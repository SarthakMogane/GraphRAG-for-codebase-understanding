{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Semantic Enrichment & Vector Search\n",
    "## Adding Intelligence to Your Code Graph\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Load the Week 1 structural graph\n",
    "2. Add LLM-generated summaries\n",
    "3. Create vector embeddings\n",
    "4. Build FAISS index\n",
    "5. Test semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "\n",
    "from utils.llm_client import LangChainClient\n",
    "from indexing.semantic_enrichment import SemanticEnricher\n",
    "from indexing.vector_store import VectorStore\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Week 1 Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph: 28 nodes, 209 edges\n"
     ]
    }
   ],
   "source": [
    "# Load the structural graph from Week 1\n",
    "graph_path = '../data/graphs/code_graph.pkl'\n",
    "\n",
    "with open(graph_path, 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: typing (import)\n",
      "\n",
      "Current attributes:\n",
      "  name: typing\n",
      "  type: import\n",
      "  code: 28 chars\n",
      "  docstring: None\n",
      "  file_path: d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\file.py\n",
      "  start_line: 6\n",
      "  end_line: 6\n",
      "  language: python\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample node BEFORE enrichment\n",
    "sample_node = list(graph.nodes(data=True))[5]\n",
    "node_id, attrs = sample_node\n",
    "\n",
    "print(f\"Node: {attrs['name']} ({attrs['type']})\")\n",
    "print(f\"\\nCurrent attributes:\")\n",
    "for key, value in attrs.items():\n",
    "    if key != 'code':\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  code: {len(value)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic Enrichment with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LLM client initialized: google\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM client\n",
    "llm_client = LangChainClient(\n",
    "    provider='google',  # or 'anthropic'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LLM client initialized: {llm_client.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SemanticEnricher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test enrichment on a single node first\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m enricher = \u001b[43mSemanticEnricher\u001b[49m(llm_client=llm_client)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Pick a function node\u001b[39;00m\n\u001b[32m      5\u001b[39m test_node = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'SemanticEnricher' is not defined"
     ]
    }
   ],
   "source": [
    "# Test enrichment on a single node first\n",
    "enricher = SemanticEnricher(llm_client=llm_client)\n",
    "\n",
    "# Pick a function node\n",
    "test_node = None\n",
    "for node_id, attrs in graph.nodes(data=True):\n",
    "    if attrs.get('type') == 'function' and len(attrs.get('code', '')) > 50:\n",
    "        test_node = (node_id, attrs)\n",
    "        break\n",
    "\n",
    "if test_node:\n",
    "    node_id, attrs = test_node\n",
    "    print(f\"Testing enrichment on: {attrs['name']}\\n\")\n",
    "    \n",
    "    enriched = enricher.enrich_single(node_id, attrs)\n",
    "    \n",
    "    print(\"âœ“ Enrichment result:\")\n",
    "    print(f\"  Summary: {enriched.summary}\")\n",
    "    print(f\"  Description: {enriched.description}\")\n",
    "    print(f\"  Tags: {', '.join(enriched.tags)}\")\n",
    "    print(f\"  Complexity: {enriched.complexity}\")\n",
    "    print(f\"  Purpose: {enriched.purpose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-19 16:38:56.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mEnriching 5 nodes...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriching all nodes... (this may take 2-5 minutes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[32m2025-12-19 16:39:02.140\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36m_enrich_node\u001b[0m:\u001b[36m139\u001b[0m - \u001b[33m\u001b[1mStructured output failed, using fallback for function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\file.py:fetch_hashed_password:17\u001b[0m\n",
      "\u001b[32m2025-12-19 16:39:07.436\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m114\u001b[0m - \u001b[31m\u001b[1mFailed to enrich function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\file.py:fetch_hashed_password:17: Error calling model 'gemini-2.5-pro' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\nPlease retry in 53.046057769s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '53s'}]}}\u001b[0m\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [00:10<00:42, 10.63s/it]\u001b[32m2025-12-19 16:39:12.625\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36m_enrich_node\u001b[0m:\u001b[36m139\u001b[0m - \u001b[33m\u001b[1mStructured output failed, using fallback for function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\file.py:fetch_user_data:39\u001b[0m\n",
      "\u001b[32m2025-12-19 16:39:18.372\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m114\u001b[0m - \u001b[31m\u001b[1mFailed to enrich function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\file.py:fetch_user_data:39: Error calling model 'gemini-2.5-pro' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\nPlease retry in 41.98600288s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}\u001b[0m\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:21<00:32, 10.81s/it]\u001b[32m2025-12-19 16:39:23.162\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36m_enrich_node\u001b[0m:\u001b[36m139\u001b[0m - \u001b[33m\u001b[1mStructured output failed, using fallback for function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\main.py:transfrom_text:12\u001b[0m\n",
      "\u001b[32m2025-12-19 16:39:28.096\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m114\u001b[0m - \u001b[31m\u001b[1mFailed to enrich function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\main.py:transfrom_text:12: Error calling model 'gemini-2.5-pro' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\nPlease retry in 32.258545359s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '32s'}]}}\u001b[0m\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:31<00:20, 10.31s/it]\u001b[32m2025-12-19 16:39:33.620\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36m_enrich_node\u001b[0m:\u001b[36m139\u001b[0m - \u001b[33m\u001b[1mStructured output failed, using fallback for function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\src\\utils\\db_connection.py:create_connection:14\u001b[0m\n",
      "\u001b[32m2025-12-19 16:39:38.409\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m114\u001b[0m - \u001b[31m\u001b[1mFailed to enrich function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\src\\utils\\db_connection.py:create_connection:14: Error calling model 'gemini-2.5-pro' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\nPlease retry in 21.946673389s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-pro', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '21s'}]}}\u001b[0m\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:41<00:10, 10.31s/it]\u001b[32m2025-12-19 16:39:42.565\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36m_enrich_node\u001b[0m:\u001b[36m139\u001b[0m - \u001b[33m\u001b[1mStructured output failed, using fallback for function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\src\\utils\\db_connection.py:get_db_connection:45\u001b[0m\n",
      "\u001b[32m2025-12-19 16:39:47.839\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m114\u001b[0m - \u001b[31m\u001b[1mFailed to enrich function:d:\\kaggle_project\\GraphRAG\\data\\repositories\\SMS-Spam-VotingClassifier-\\src\\utils\\db_connection.py:get_db_connection:45: Error calling model 'gemini-2.5-pro' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.5-pro\\nPlease retry in 12.515554189s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerDay-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-pro'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '12s'}]}}\u001b[0m\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:51<00:00, 10.21s/it]\n",
      "\u001b[32m2025-12-19 16:39:47.849\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mindexing.semantic_enrichment\u001b[0m:\u001b[36menrich_graph\u001b[0m:\u001b[36m117\u001b[0m - \u001b[32m\u001b[1mEnrichment complete: 0 enriched, 5 failed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Enrichment complete!\n"
     ]
    }
   ],
   "source": [
    "# Enrich the entire graph (this will take a few minutes)\n",
    "print(\"Enriching all nodes... (this may take 2-5 minutes)\\n\")\n",
    "\n",
    "enriched_graph = enricher.enrich_graph(graph, skip_existing=True)\n",
    "\n",
    "print(\"\\nâœ“ Enrichment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Enrichment Statistics ===\n",
      "Total nodes: 28\n",
      "Enriched: 0 (0.0%)\n",
      "Unique tags: 0\n",
      "\n",
      "Top tags:\n"
     ]
    }
   ],
   "source": [
    "# Show enrichment statistics\n",
    "stats = enricher.get_enrichment_stats(enriched_graph)\n",
    "\n",
    "print(f\"\\n=== Enrichment Statistics ===\")\n",
    "print(f\"Total nodes: {stats['total_nodes']}\")\n",
    "print(f\"Enriched: {stats['enriched_nodes']} ({stats['enrichment_rate']:.1%})\")\n",
    "print(f\"Unique tags: {stats['total_unique_tags']}\")\n",
    "\n",
    "print(f\"\\nTop tags:\")\n",
    "for tag, count in stats['top_tags'][:10]:\n",
    "    print(f\"  {tag}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize tag distribution\u001b[39;00m\n\u001b[32m      2\u001b[39m tag_data = stats[\u001b[33m'\u001b[39m\u001b[33mtop_tags\u001b[39m\u001b[33m'\u001b[39m][:\u001b[32m15\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tags, counts = \u001b[38;5;28mzip\u001b[39m(*tag_data)\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      6\u001b[39m plt.barh(tags, counts, color=\u001b[33m'\u001b[39m\u001b[33mskyblue\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Visualize tag distribution\n",
    "tag_data = stats['top_tags'][:15]\n",
    "tags, counts = zip(*tag_data)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(tags, counts, color='skyblue')\n",
    "plt.xlabel('Count')\n",
    "plt.title('Top 15 Code Tags')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LangChainClient.get_stats() missing 1 required positional argument: 'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# LLM usage statistics\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm_stats = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== LLM Usage Statistics ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal requests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_stats[\u001b[33m'\u001b[39m\u001b[33mtotal_requests\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: LangChainClient.get_stats() missing 1 required positional argument: 'response'"
     ]
    }
   ],
   "source": [
    "# LLM usage statistics\n",
    "llm_stats = llm_client.get_stats()\n",
    "\n",
    "print(\"\\n=== LLM Usage Statistics ===\")\n",
    "print(f\"Total requests: {llm_stats['total_requests']}\")\n",
    "print(f\"Total tokens: {llm_stats['total_tokens']:,}\")\n",
    "print(f\"Cache hits: {llm_stats['cache_hits']} ({llm_stats['cache_hit_rate']:.1%})\")\n",
    "print(f\"Avg tokens/request: {llm_stats['avg_tokens_per_request']:.0f}\")\n",
    "\n",
    "# Estimate cost (rough)\n",
    "if 'gpt-4' in llm_client.model:\n",
    "    cost_per_1k = 0.01  # Approximate\n",
    "    estimated_cost = (llm_stats['total_tokens'] / 1000) * cost_per_1k\n",
    "    print(f\"\\nEstimated cost: ${estimated_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = VectorStore(\n",
    "    model_name='mini',  # Fast and good quality\n",
    "    index_type='Flat'   # Exact search\n",
    ")\n",
    "\n",
    "print(f\"Vector store initialized:\")\n",
    "print(f\"  Embedding dim: {vector_store.embedding_dim}\")\n",
    "print(f\"  Index type: {vector_store.index_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index from enriched graph\n",
    "print(\"Building vector index...\\n\")\n",
    "\n",
    "vector_store.build_from_graph(\n",
    "    enriched_graph,\n",
    "    text_field='combined',  # Combines code, summary, tags\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Vector index built: {len(vector_store.node_ids)} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_search_results(query, results):\n",
    "    \"\"\"Pretty print search results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result.name} ({result.type})\")\n",
    "        print(f\"   Score: {result.score:.3f}\")\n",
    "        if result.summary:\n",
    "            print(f\"   Summary: {result.summary}\")\n",
    "        if result.tags:\n",
    "            print(f\"   Tags: {', '.join(result.tags[:5])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m test_queries = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfunction that validates user input\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcode for reading and parsing data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatabase connection or storage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration and settings\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m test_queries:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     results = \u001b[43mvector_store\u001b[49m.search(query, top_k=\u001b[32m3\u001b[39m)\n\u001b[32m     11\u001b[39m     display_search_results(query, results)\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "# Test various queries\n",
    "test_queries = [\n",
    "    \"function that validates user input\",\n",
    "    \"code for reading and parsing data\",\n",
    "    \"database connection or storage\",\n",
    "    \"configuration and settings\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = vector_store.search(query, top_k=3)\n",
    "    display_search_results(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search\n",
    "def interactive_search():\n",
    "    print(\"\\nðŸ” Enter queries (or 'quit' to exit):\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Query > \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        results = vector_store.search(query, top_k=5)\n",
    "        display_search_results(query, results)\n",
    "\n",
    "# Uncomment to use:\n",
    "# interactive_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Search Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m query_labels = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m sample_queries:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     results = \u001b[43mvector_store\u001b[49m.search(query, top_k=\u001b[32m10\u001b[39m)\n\u001b[32m     13\u001b[39m     scores = [r.score \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m     14\u001b[39m     all_scores.extend(scores)\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare score distributions\n",
    "sample_queries = [\n",
    "    \"validation function\",\n",
    "    \"data processing\",\n",
    "    \"configuration management\"\n",
    "]\n",
    "\n",
    "all_scores = []\n",
    "query_labels = []\n",
    "\n",
    "for query in sample_queries:\n",
    "    results = vector_store.search(query, top_k=10)\n",
    "    scores = [r.score for r in results]\n",
    "    all_scores.extend(scores)\n",
    "    query_labels.extend([query[:20]] * len(scores))\n",
    "\n",
    "# Plot\n",
    "df = pd.DataFrame({'Query': query_labels, 'Score': all_scores})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x='Query', y='Score')\n",
    "plt.title('Search Score Distribution by Query Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'save_to_file' not found\n"
     ]
    }
   ],
   "source": [
    "# Find similar nodes\n",
    "def find_and_display_similar(node_name, top_k=5):\n",
    "    \"\"\"Find nodes similar to a given node\"\"\"\n",
    "    \n",
    "    # Find node ID\n",
    "    target_node_id = None\n",
    "    for node_id, attrs in enriched_graph.nodes(data=True):\n",
    "        if attrs.get('name') == node_name:\n",
    "            target_node_id = node_id\n",
    "            print(f\"Found: {attrs['name']} ({attrs['type']})\")\n",
    "            print(f\"Summary: {attrs.get('summary', 'N/A')}\\n\")\n",
    "            break\n",
    "    \n",
    "    if not target_node_id:\n",
    "        print(f\"Node '{node_name}' not found\")\n",
    "        return\n",
    "    \n",
    "    # Find similar\n",
    "    similar = vector_store.find_similar_nodes(target_node_id, top_k=top_k)\n",
    "    \n",
    "    print(f\"Similar nodes:\\n\")\n",
    "    for i, result in enumerate(similar, 1):\n",
    "        print(f\"{i}. {result.name} ({result.type}) - Score: {result.score:.3f}\")\n",
    "        if result.summary:\n",
    "            print(f\"   {result.summary}\")\n",
    "        print()\n",
    "\n",
    "# Try it\n",
    "find_and_display_similar('save_to_file', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enriched graph\n",
    "enriched_path = '../data/graphs/code_graph_enriched.pkl'\n",
    "with open(enriched_path, 'wb') as f:\n",
    "    pickle.dump(enriched_graph, f)\n",
    "print(f\"âœ“ Enriched graph saved: {enriched_path}\")\n",
    "\n",
    "# Save vector store\n",
    "vector_path = '../data/graphs/vector_store'\n",
    "vector_store.save(vector_path)\n",
    "print(f\"âœ“ Vector store saved: {vector_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Week 2 Complete! You now have:\n",
    "- âœ… LLM-generated summaries for all code entities\n",
    "- âœ… Domain tags and complexity ratings\n",
    "- âœ… Vector embeddings (384 dimensions)\n",
    "- âœ… FAISS index for fast similarity search\n",
    "- âœ… Semantic search capability\n",
    "\n",
    "**Week 3 Preview:**\n",
    "- Community detection (Louvain algorithm)\n",
    "- Hierarchical summarization\n",
    "- Global query support\n",
    "- Query classification (global vs local)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
